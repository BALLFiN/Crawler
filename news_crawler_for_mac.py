# producer.py (ì „ì²´ ì½”ë“œ)

import requests
from bs4 import BeautifulSoup
import newspaper
import time
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin
from kafka import KafkaProducer
import json
import re
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed

# ----------------- ì„¤ì • íŒŒì¼ import -----------------
# ì•„ë˜ íŒŒì¼ë“¤ì´ .pyì™€ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from news_list import TARGET_SITES, CHECK_INTERVAL_SECONDS, STOCK_CODE 
from identify_company_module import identify_company 

# ----------------- ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì • -----------------
KST = timezone(timedelta(hours=9))
SCRIPT_START_TIME = datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(KST)
recent_titles = deque(maxlen=100)
processed_urls = deque(maxlen=100)
last_seen_urls = {f"{site['name']} - {site.get('category', '')}": None for site in TARGET_SITES}

# ----------------- Kafka Producer ì„¤ì • -----------------
# âš ï¸ ì¤‘ìš”: Kafka ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ PCì˜ IP ì£¼ì†Œë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
# ë§Œì•½ ë§¥ì—ì„œ Kafka ì„œë²„ë¥¼ ì‹¤í–‰ ì¤‘ì´ë¼ë©´, 'localhost' ëŒ€ì‹  ë§¥ì˜ ë‚´ë¶€ IP ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”.
# ì˜ˆ: bootstrap_servers=['192.168.1.10:9092']
try:
    producer = KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
    )
    print("âœ… Kafka Producerì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ Kafka Producer ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. bootstrap_servers ì£¼ì†Œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    print(f"ì˜¤ë¥˜: {e}")
    exit()

# ----------------- ì„¸ì…˜ ë° í—¬í¼ í•¨ìˆ˜ -----------------
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
})

def localize_to_kst(dt):
    if dt is None: return None
    if dt.tzinfo is None: return dt.replace(tzinfo=timezone.utc).astimezone(KST)
    return dt.astimezone(KST)

def fetch_article_details(url):
    try:
        article = newspaper.Article(url, language='ko')
        article.download()
        article.parse()
        return {
            'title': article.title,
            'text': article.text,
            'publish_time': localize_to_kst(article.publish_date),
        }
    except Exception as e:
        print(f"[ERROR] ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {url}\n  â”” {e}")
        return None

# ----------------- ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬ í•¨ìˆ˜ -----------------
def get_latest_articles(site):
    try:
        res = session.get(site['url'], timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')

        container = soup.select_one(site.get("container_selector", "body"))
        if not container:
            # print(f"[INFO] {site['name']} â†’ container_selector ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. body ì „ì²´ì—ì„œ íƒìƒ‰í•©ë‹ˆë‹¤.")
            container = soup
        
        links = container.select(site["selector"])
        articles = []
        for a in links:
            href = a.get('href')
            if not href or site.get('pattern', '') not in href:
                continue

            # ì œëª© ì¶”ì¶œ ë¡œì§ (ì–¸ë¡ ì‚¬ë³„ ëŒ€ì‘)
            title = a.get_text(strip=True)
            if site['name'] == "ë§¤ì¼ê²½ì œ":
                title_tag = a.select_one("h3.news_ttl")
                if title_tag: title = title_tag.get_text(strip=True)
            elif site['name'] == "í—¤ëŸ´ë“œê²½ì œ":
                title_tag = a.select_one("p.news_title")
                if title_tag: title = title_tag.get_text(strip=True)
            elif site['name'] == "í•œê²¨ë ˆ":
                title_tag = a.select_one("div.BaseArticleCard_title__TVFqt")
                if title_tag: title = title_tag.get_text(strip=True)
                
            full_url = urljoin(site['base_url'], href)
            
            img_url = None

            # ë§¤ì¼ê²½ì œ ì „ìš© ì²˜ë¦¬
            if site['name'] == "ë§¤ì¼ê²½ì œ":
                parent_container = a.find_parent('li', class_='news_node')
                if parent_container:
                    img_tag = parent_container.select_one('div.thumb_area > img')
                    if img_tag:
                        if img_tag.has_attr('data-src') and img_tag['data-src'].strip():
                            img_url = img_tag['data-src']
                        elif img_tag.has_attr('src') and img_tag['src'].strip():
                            img_url = img_tag['src']
                        img_url = urljoin(site['base_url'], img_url)
                # ë§¤ì¼ê²½ì œ ì „ìš© ì²˜ë¦¬ í›„ ì¼ë°˜ ë¡œì§ì€ íƒ€ì§€ ì•ŠìŒ
            else:
                # ì¼ë°˜ ì´ë¯¸ì§€ ì¶”ì¶œ ë¡œì§
                parent_container = a.find_parent(['li', 'div', 'article'])
                img_tag = None
                if parent_container:
                    img_tag = parent_container.select_one(site['image_selector'])
                if not img_tag:
                    img_tag = a.select_one('img')
                if not img_tag:
                    img_tag = soup.select_one(f"a[href='{href}'] img")
                if img_tag:
                    img_url = img_tag.get('data-src') or img_tag.get('src')
                    if img_url:
                        img_url = urljoin(site['base_url'], img_url)

            # 4ï¸âƒ£ ë°±ì—…: data-share-img ì†ì„± í™•ì¸
            if not img_url:
                share_div = parent_container.select_one("div[class^='share-data-']")
                if share_div and share_div.has_attr("data-share-img"):
                    img_url = share_div["data-share-img"]


            articles.append({'title': title, 'url': full_url, 'image': img_url})
        return articles
    except Exception as e:
        print(f"[ERROR] ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨ ({site['name']}) â†’ {e}")
        return []

def send_article_to_kafka(article_data):
    """kafka-pythonì„ ì‚¬ìš©í•˜ì—¬ ì»¨ìŠˆë¨¸ê°€ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ë°œí–‰"""
    try:
        producer.send("news", value=article_data)
        print(f"ğŸš€ ë©”ì‹œì§€ ë°œí–‰: [{article_data['company']}] {article_data['title'][:40]}...")
    except Exception as e:
        print(f"[ERROR] Kafka ë©”ì‹œì§€ ë°œí–‰ ì‹¤íŒ¨: {e}")

# ----------------- ë©”ì¸ ëª¨ë‹ˆí„°ë§ ë¡œì§ -----------------
def monitor_news():
    print("\n[ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘]")
    print(f"  - Kafka Server: {producer.config['bootstrap_servers']}")
    first_run = True

    while True:
        with ThreadPoolExecutor(max_workers=8) as executor:
            future_to_site = {executor.submit(get_latest_articles, site): site for site in TARGET_SITES}
            for future in as_completed(future_to_site):
                site = future_to_site[future]
                site_key = f"{site['name']} - {site.get('category', '')}"
                
                try:
                    article_list = future.result()
                    if not article_list: continue

                    top_article = article_list[0]
                    top_url = top_article['url']
                    
                    if first_run:
                        last_seen_urls[site_key] = top_url
                        # print(f"  ğŸ”¹ [{site_key}] ì´ˆê¸° ê¸°ì¤€ ê¸°ì‚¬ ì„¤ì • ì™„ë£Œ.")
                        continue

                    if top_url == last_seen_urls[site_key]: continue
                    
                    # ìƒˆ ê¸°ì‚¬ ë°œê²¬ ì‹œ ë¡œì§
                    last_seen_urls[site_key] = top_url
                    if top_article['title'] in recent_titles or top_url in processed_urls: continue
                    
                    print(f"\nâœ¨ ìƒˆ ê¸°ì‚¬ ë°œê²¬: [{site['name']}] {top_article['title']}")
                    recent_titles.append(top_article['title'])
                    processed_urls.append(top_url)
                    
                    details = fetch_article_details(top_url)
                    if not details: continue
                    
                    # ê¸°ì—…ëª… íƒœê¹…
                    company_name, _, _ = identify_company(details['title'], details['text'])
                    company_code = STOCK_CODE.get(company_name)
                    
                    # ìœ íš¨í•œ íšŒì‚¬ ì½”ë“œê°€ ì‹ë³„ëœ ê²½ìš°ì—ë§Œ ì¹´í”„ì¹´ë¡œ ì „ì†¡
                    if not company_code:
                        print(f"  âš ï¸ ê¸°ì‚¬ ê±´ë„ˆëœ€ (ê´€ë ¨ ê¸°ì—… ì½”ë“œ ì—†ìŒ)")
                        continue
                    
                    pub_time = details.get('publish_time') or datetime.now(KST)

                    # ì»¨ìŠˆë¨¸ê°€ ê¸°ëŒ€í•˜ëŠ” ìµœì¢… ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ì¡°ë¦½
                    article_data = {
                        "company": company_name,
                        "code": company_code,
                        "link": top_url,
                        "press": site['name'],
                        "published_at": pub_time.strftime('%Y-%m-%d %H:%M:%S'),
                        "title": details['title'],
                        "image_url": top_article.get('image'),
                        "content": details['text']
                    }
                    
                    send_article_to_kafka(article_data)

                except Exception as e:
                    print(f"[ERROR] {site['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if first_run:
            print("\nì´ˆê¸° ê¸°ì¤€ ì„¤ì •ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆë¶€í„° ìƒˆë¡œìš´ ë‰´ìŠ¤ë¥¼ ê°ì‹œí•©ë‹ˆë‹¤.")
            first_run = False
            
        print(f"\n[{datetime.now(KST).strftime('%H:%M:%S')}] ë‹¤ìŒ ì²´í¬ê¹Œì§€ {CHECK_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°...")
        time.sleep(CHECK_INTERVAL_SECONDS)

# ----------------- í”„ë¡œê·¸ë¨ ì‹œì‘ -----------------
if __name__ == "__main__":
    try:
        monitor_news()
    except KeyboardInterrupt:
        print("\n\n[í”„ë¡œê·¸ë¨ ì¢…ë£Œ] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨.")
    except Exception as e:
        print(f"\n[ì¹˜ëª…ì  ì˜¤ë¥˜] ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤: {e}")
    finally:
        print("\n[ì •ë¦¬] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        if 'producer' in locals() and producer:
            producer.flush(timeout=10)
            producer.close()
            print("  - Kafka Producer ì—°ê²° ì¢…ë£Œ ì™„ë£Œ")
        print("í”„ë¡œê·¸ë¨ì„ ì™„ì „íˆ ì¢…ë£Œí•©ë‹ˆë‹¤.")