# dify_knowledge.py
import os
import json
import time
import hashlib
from datetime import date
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

import _config
from dify_api import DifyClient
from datetime import datetime, timedelta, date
import re

# ------------------------------- ê³µìš© ìœ í‹¸ -------------------------------

STATE_PATH = os.path.join(os.path.dirname(__file__), "state.json")


def load_state():
    """ì¦ë¶„ ìˆ˜ì§‘ì„ ìœ„í•œ ë¡œì»¬ ìƒíƒœ ë¡œë“œ"""
    if os.path.exists(STATE_PATH):
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    # ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ
    return {
        "industry": {"last_key": None},
        "corporate": {"last_key": None},
        "macro": {"last_date": None},
        "seen": {}
    }


def save_state(state):
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def make_doc_key(type_tag: str, category: str, date_str: str, src_name: str = "") -> str:
    """
    ì—…ë¡œë“œ í•­ëª©ì˜ ìœ ë‹ˆí¬ í‚¤(í•´ì‹œ) ìƒì„±.
    type|category|date|src ë¡œ êµ¬ì„± â†’ 16ìë¦¬ í•´ì‹œ
    """
    raw = f"{type_tag}|{category}|{date_str}|{src_name}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()[:16]


def tag_prefix(_type: str, **meta) -> str:
    """
    íƒœê·¸ í”„ë¦¬í”½ìŠ¤ ìƒì„±
    ì˜ˆ) tag_prefix('industry', CODE='GICS-4510', COUNTRY='KR')
       -> "[TYPE=industry][CODE=GICS-4510][COUNTRY=KR]"
    """
    tags = [f"[TYPE={_type}]"]
    for k, v in meta.items():
        if v is None or v == "":
            continue
        tags.append(f"[{k.upper()}={v}]")
    return "".join(tags)


def get_page(url: str):
    try:
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
        resp.raise_for_status()
        return BeautifulSoup(resp.text, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"[ì˜¤ë¥˜] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {url} ({e})")
        return None


# --------------------------- ì˜¤ë˜ëœ ì •ë³´ ì‚¬ì „ ì‚­ì œ ---------------------------


# ë‚ ì§œ íŒ¨í„´
DATE_PATTERNS = [
    re.compile(r"\[DATE=(\d{4}-\d{2}-\d{2})\]"),     # [DATE=YYYY-MM-DD]
    re.compile(r"(\d{4}[.-]\d{2}[.-]\d{2})"),        # YYYY.MM.DD or YYYY-MM-DD
]

def _extract_date_from_name(name: str):
    """ë¬¸ì„œëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ â†’ datetime.date or None"""
    for pat in DATE_PATTERNS:
        m = pat.search(name)
        if m:
            s = m.group(1).replace(".", "-")
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except ValueError:
                pass
    return None


def cleanup_old_documents(dify_client, knowledge_id: str):
    """
    KB ë¬¸ì„œ ëª©ë¡ ì¤‘ ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ:
    - [TYPE=macro] â†’ 1ì¼ ì´ˆê³¼ ì‚­ì œ
    - [TYPE=industry] / [TYPE=company] â†’ 7ì¼ ì´ˆê³¼ ì‚­ì œ
    """
    print("\nğŸ§¹ ì˜¤ë˜ëœ ë¬¸ì„œ ì •ë¦¬ ì¤‘...")
    try:
        docs = dify_client.list_all_documents(knowledge_id)
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return

    today = date.today()
    deleted_count = 0

    for doc in docs:
        name = doc.get("name", "")
        doc_id = doc.get("id")
        d = _extract_date_from_name(name)
        if not d:
            continue  # ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨ â†’ ë³´ì¡´

        age = (today - d).days
        # ê±°ì‹œê²½ì œ: í•˜ë£¨ ì´ˆê³¼ ì‚­ì œ
        if "[TYPE=macro]" in name and age >= 1:
            dify_client.delete_document(knowledge_id, doc_id)
            print(f"ğŸ—‘ï¸ [ê±°ì‹œê²½ì œ] {name} ({age}ì¼ ê²½ê³¼) ì‚­ì œ")
            deleted_count += 1
        # ì‚°ì—…/ê¸°ì—… ë¦¬í¬íŠ¸: 7ì¼ ì´ˆê³¼ ì‚­ì œ
        elif ("[TYPE=industry]" in name or "[TYPE=company]" in name or "[TYPE=corporate]" in name) and age > 7:
            dify_client.delete_document(knowledge_id, doc_id)
            print(f"ğŸ—‘ï¸ [ë¦¬í¬íŠ¸] {name} ({age}ì¼ ê²½ê³¼) ì‚­ì œ")
            deleted_count += 1

    print(f"âœ… ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ë¬¸ì„œ ì‚­ì œ\n")
# --------------------------- ë¦¬í¬íŠ¸(ì‚°ì—…/ê¸°ì—…) ìˆ˜ì§‘ ---------------------------

def scrape_and_upload_reports(
    dify_client: DifyClient,
    knowledge_id: str,
    base_url: str,
    max_pages: int,
    job_name: str,
    type_tag: str,
    state: dict
):
    """
    ë„¤ì´ë²„ê¸ˆìœµ ë¦¬í¬íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¦ë¶„ ìˆ˜ì§‘ í›„ integration_KBì— ì—…ë¡œë“œ.
    - ìµœì‹  í˜ì´ì§€ë¶€í„° ìˆœíšŒ
    - state['<type>'].last_key ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
    - state['seen'][doc_key] ì¡´ì¬ ì‹œ ìŠ¤í‚µ
    - íŒŒì¼ ì—…ë¡œë“œ + ê²€ìƒ‰ íŒíŠ¸ í…ìŠ¤íŠ¸ ì—…ë¡œë“œ(ì§§ì€ TAGS ë¬¸ì„œ)
    """
    print(f"\n{'='*60}\nğŸ’¼ {job_name} ì¦ë¶„ ìˆ˜ì§‘ ì‹œì‘... (KB: {knowledge_id})\n{'='*60}")

    last_anchor = state.get(type_tag, {}).get("last_key")
    first_new_key = None
    hit_old_anchor = False
    uploaded = 0

    for page in range(1, max_pages + 1):
        if hit_old_anchor:
            break

        url = f"{base_url}?&page={page}"
        print(f" - ëª©ë¡ í˜ì´ì§€: {url}")
        soup = get_page(url)
        if not soup:
            continue

        rows = soup.select("table.type_1 tr")
        if not rows or len(rows) <= 1:
            continue

        # í—¤ë” ì œê±° í›„ ë³¸ë¬¸
        for row in rows[1:]:
            cols = row.find_all("td")
            if len(cols) < 6:
                continue

            category = cols[0].get_text(strip=True)  # ì—…ì¢…ëª…/ì¢…ëª©ëª…
            date_str = cols[4].get_text(strip=True)  # ì˜ˆ: 2025.01.03
            a = cols[3].find("a")
            if not a or not a.has_attr("href"):
                continue

            src_url = urljoin("https://finance.naver.com", a["href"])
            src_name = src_url.split("/")[-1].split("?")[0]

            doc_key = make_doc_key(type_tag, category, date_str, src_name)

            # ì•µì»¤(ë§ˆì§€ë§‰ ì²˜ë¦¬ ì§€ì )ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
            if last_anchor and doc_key == last_anchor:
                hit_old_anchor = True
                print("   Â· ì´ì „ ê¸°ì¤€ì  ë„ë‹¬ â†’ ì´ë²ˆ ìˆ˜ì§‘ ì¢…ë£Œ")
                break

            # ì´ë¯¸ ì—…ë¡œë“œí•œ ë¬¸ì„œë©´ ìŠ¤í‚µ
            if state["seen"].get(doc_key):
                continue

            # ìƒˆ ë¬¸ì„œë©´ ì²« ë²ˆì§¸ í‚¤ë¥¼ ê¸°ë¡(ì´ê²Œ ìƒˆë¡œìš´ ê¸°ì¤€ì )
            if not first_new_key:
                first_new_key = doc_key

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            try:
                f_resp = requests.get(src_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=30)
                f_resp.raise_for_status()
                file_content = f_resp.content
            except requests.exceptions.RequestException as e:
                print(f"   Â· [ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨] {src_url} ({e})")
                continue

            # ì—…ë¡œë“œ (íŒŒì¼ ì œëª©ì—ë„ TYPE íƒœê·¸ ë„£ì–´ ê²€ìƒ‰ ë³´ê°•)
            tagged_name = f"{tag_prefix(type_tag)} {category}-{date_str}"
            ok = dify_client.upload_file_to_dify(tagged_name, file_content, src_name, knowledge_id)
            if ok:
                uploaded += 1
                state["seen"][doc_key] = True
                print(f"   Â· ì—…ë¡œë“œ ì™„ë£Œ: {tagged_name}")

            # ì•„ì£¼ ì§§ì€ íŒíŠ¸ í…ìŠ¤íŠ¸ë„ í•¨ê»˜ ì—…ë¡œë“œ(ê²€ìƒ‰ ê°•í™”ë¥¼ ìœ„í•´)
            hint_text = f"{tag_prefix(type_tag, NAME=category, DATE=date_str)}\nsource_file: {src_name}"
            dify_client.upload_text_to_dify(f"{tagged_name} [TAGS]", hint_text, knowledge_id)

            time.sleep(0.6)

    # ìƒˆ ê¸°ì¤€ì  ê°±ì‹ 
    if first_new_key:
        state.setdefault(type_tag, {})["last_key"] = first_new_key

    print(f"âœ… {job_name} ì™„ë£Œ: ì—…ë¡œë“œ {uploaded}ê±´\n{'='*60}\n")


# --------------------------- ê±°ì‹œê²½ì œ ìƒì„± & ì—…ë¡œë“œ ---------------------------

def run_macro_analysis_and_upload(
    dify_client: DifyClient,
    knowledge_id: str,
    perplexity_key: str,
    state: dict
):
    """
    Perplexityë¡œ ê±°ì‹œê²½ì œ ìš”ì•½ ìƒì„± í›„ integration_KBì— í…ìŠ¤íŠ¸ ì—…ë¡œë“œ.
    - ë‹¹ì¼ ì´ë¯¸ ì˜¬ë ¸ë‹¤ë©´ ìŠ¤í‚µ
    - ë³¸ë¬¸ ë§¨ ì•ì— [TYPE=macro][DATE=YYYY-MM-DD] ì‚½ì…
    - í”„ë¡¬í”„íŠ¸ëŠ” ì‚¬ìš©ì ìš”ì²­ëŒ€ë¡œ 'ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•ŠìŒ'
    """
    today = date.today().strftime('%Y-%m-%d')
    if state.get("macro", {}).get("last_date") == today:
        print(f"ğŸŒ ê±°ì‹œê²½ì œ: ì´ë¯¸ ì˜¤ëŠ˜({today}) ì—…ë¡œë“œë˜ì–´ ìŠ¤í‚µ")
        return

    print(f"\n{'='*60}\nğŸŒ ê±°ì‹œê²½ì œ ë¶„ì„ ìƒì„± ì‹œì‘...\n{'='*60}")

    # â›” í”„ë¡¬í”„íŠ¸ëŠ” ì‚¬ìš©ìê°€ ë°”ê¾¸ì§€ ë§ë¼ê³  í•œ ê·¸ëŒ€ë¡œ ë‘ 
    prompt_content = """
    ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ê²½ì œ ë¶„ì„ê°€ë‹¤. ì˜¤ëŠ˜ì˜ ê²½ì œ ë™í–¥ì„ CEO ì•„ì¹¨ ë³´ê³  í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ë¼.
    Part 1: í•µì‹¬ ì§€í‘œ ëŒ€ì‹œë³´ë“œ(í…ìŠ¤íŠ¸ ë‚˜ì—´)
    Part 2: Top ì´ìŠˆ ì‹¬ì¸µ ë¶„ì„(ì˜í–¥/ì‚°ì—…ë³„ ëª…ì•”/ì‹¤í–‰ ì œì–¸)
    """

    try:
        resp = requests.post(
            'https://api.perplexity.ai/chat/completions',
            headers={'Authorization': f'Bearer {perplexity_key}', 'Content-Type': 'application/json'},
            json={
                'model': 'sonar-pro',
                'messages': [{'role': 'user', 'content': prompt_content}]
            },
            timeout=60
        )
        if resp.status_code != 200:
            print(f"âŒ Perplexity API ì‹¤íŒ¨: {resp.status_code} - {resp.text}")
            return

        result = resp.json()
        content = result['choices'][0]['message']['content']
        print("âœ… ê±°ì‹œê²½ì œ ë¶„ì„ ìˆ˜ì‹ ")

        header = tag_prefix('macro', DATE=today)
        tagged_text = f"{header}\n\n{content}"
        doc_name = f"{header} ë°ì¼ë¦¬ ê±°ì‹œê²½ì œ ë³´ê³ ì„œ - {today}"
        dify_client.upload_text_to_dify(doc_name, tagged_text, knowledge_id)

        # ìƒíƒœ ê°±ì‹ 
        state.setdefault("macro", {})["last_date"] = today
        print(f"âœ… ê±°ì‹œê²½ì œ ì—…ë¡œë“œ ì™„ë£Œ: {doc_name}")

    except Exception as e:
        print(f"âŒ ê±°ì‹œê²½ì œ ìƒì„±/ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")

    print(f"{'='*60}\n")


# --------------------------------- ë©”ì¸ ---------------------------------

def main():
    print("ğŸš€ integration_KB ì—…ë°ì´íŠ¸ ì‹œì‘")

    # ---- ì„¤ì • ë¡œë“œ
    API_KEY = _config.DIFY_API_KEY
    HOST = _config.DIFY_HOST
    PERPLEXITY_KEY = _config.perplexity_api
    INTEGRATION_ID = _config.INTEGRETION_KNOWLEDGE_ID  # âœ… ë‹¨ì¼ KBë§Œ ì‚¬ìš©

    URLS = {
        "industry": _config.INDUSTRY_BASE_URL,   # ì˜ˆ: 'https://finance.naver.com/research/industry_list.naver'
        "corporate": _config.CORP_BASE_URL       # ì˜ˆ: 'https://finance.naver.com/research/company_list.naver'
    }
    MAX_PAGES = {"industry": 3, "corporate": 5}  # í•„ìš”ì‹œ ì¡°ì ˆ

    client = DifyClient(api_key=API_KEY, host=HOST, knowledge_ids={"integration": INTEGRATION_ID})
    state = load_state()

    try:
        # --- ì‚°ì—… ë¦¬í¬íŠ¸ (TYPE=industry) ---
        scrape_and_upload_reports(
            dify_client=client,
            knowledge_id=INTEGRATION_ID,
            base_url=URLS["industry"],
            max_pages=MAX_PAGES["industry"],
            job_name="ì‚°ì—… ë¦¬í¬íŠ¸",
            type_tag="industry",
            state=state
        )

        # --- ê¸°ì—… ë¦¬í¬íŠ¸ (TYPE=company) ---
        scrape_and_upload_reports(
            dify_client=client,
            knowledge_id=INTEGRATION_ID,
            base_url=URLS["corporate"],
            max_pages=MAX_PAGES["corporate"],
            job_name="ê¸°ì—… ë¦¬í¬íŠ¸",
            type_tag="corporate",   # ë‚´ë¶€ í‚¤ëŠ” 'corporate'ë¡œ, íƒœê·¸ëŠ” companyë¡œ ë‚´ë ¤ë„ ë¨
            state=state
        )

        # íƒœê·¸ëŠ” companyë¡œ í‘œì‹œë˜ê¸¸ ì›í•˜ë©´ ì—¬ê¸°ì„œ ë³´ì •
        # (ìœ„ì—ì„œ type_tag='corporate'ë¡œ ëŒë ¸ìœ¼ë‹ˆ, ê²€ìƒ‰ì—ì„œ [TYPE=company]ë¡œ ì“°ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë¼ì¸ë§Œ ë°”ê¾¸ë©´ ë¨)
        # â†’ í•„ìš” ì‹œ type_tag='company' ë¡œ ë°”ê¿” ì‚¬ìš©í•˜ì„¸ìš”.

        # --- ê±°ì‹œê²½ì œ (TYPE=macro) ---
        run_macro_analysis_and_upload(
            dify_client=client,
            knowledge_id=INTEGRATION_ID,
            perplexity_key=PERPLEXITY_KEY,
            state=state
        )

    finally:
        save_state(state)
        print("ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ")
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì¢…ë£Œ")


if __name__ == "__main__":
    main()
