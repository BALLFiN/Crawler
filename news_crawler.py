import requests
from bs4 import BeautifulSoup
import newspaper
import time
import difflib
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin, urlparse, urlunparse
from news_list import TARGET_SITES, CHECK_INTERVAL_SECONDS, MONGO_URI, MONGO_DB_NAME, MONGO_COLLECTION_NAME, STOCK_CODE
from pymongo import MongoClient
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from identify_company_module import identify_company  # ì‹¤ì œ ëª¨ë“ˆëª…ì— ë§ê²Œ ìˆ˜ì •
from confluent_kafka import Producer  # confluent-kafka ì‚¬ìš©
import json
import re

maxlen = 100

# ì„¤ì •
KST = timezone(timedelta(hours=9))
SCRIPT_START_TIME = datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(KST)
recent_titles = deque(maxlen=maxlen)
processed_urls = deque(maxlen=maxlen)

# Kafka Producer ì„¤ì • (confluent-kafka ë°©ì‹)
kafka_config = {
    'bootstrap.servers': 'localhost:9092',
    'client.id': 'news-monitor-producer',
    'compression.type': 'gzip',
    'batch.size': 16384,
    'linger.ms': 10,
}

producer = Producer(kafka_config)

def delivery_callback(err, msg):
    """Kafka ë©”ì‹œì§€ ì „ì†¡ ê²°ê³¼ ì½œë°±"""
    if err is not None:
        print(f"[ERROR] Kafka ë°œí–‰ ì‹¤íŒ¨: {err}")
    else:
        print(f"  [Kafka ë°œí–‰ ì„±ê³µ] Topic: {msg.topic()}, Partition: {msg.partition()}")

session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
    'Referer': 'https://dealsite.co.kr/',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko,en-US;q=0.9,en;q=0.8',
    'Connection': 'keep-alive'
})

last_seen_urls = {
    f"{site['name']} - {site.get('category', '')}": None for site in TARGET_SITES
}

client = MongoClient(MONGO_URI)
db = client[MONGO_DB_NAME]
articles_collection = db[MONGO_COLLECTION_NAME]

def localize_to_kst(dt):
    if dt is None:
        return None
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc).astimezone(KST)
    return dt.astimezone(KST)

def clean_url(url):
    parsed = urlparse(url)
    return urlunparse(parsed._replace(query=""))

def fetch_article_details(url, site=None):
    try:
        article = newspaper.Article(url, language='ko')
        article.download()
        article.parse()
        publish_time = localize_to_kst(article.publish_date)

        return {
            'title': article.title,
            'text': article.text,
            'publish_time': publish_time,
            'url': url
        }

    except Exception as e:
        print(f"[ERROR] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {url}\n  â”” {e}")
        return None

def send_article_to_kafka(article_data):
    """confluent-kafkaë¥¼ ì‚¬ìš©í•œ ë©”ì‹œì§€ ë°œí–‰"""
    try:
        # JSON ì§ë ¬í™”
        message_value = json.dumps(article_data, ensure_ascii=False).encode('utf-8')
        
        # ë©”ì‹œì§€ ë°œí–‰ (ë¹„ë™ê¸°)
        producer.produce(
            topic='news',
            value=message_value,
            callback=delivery_callback
        )
        
        # ë²„í¼ í”ŒëŸ¬ì‹œ (ì¦‰ì‹œ ì „ì†¡)
        producer.flush(timeout=5)
        
        print(f"  [Kafka ë°œí–‰] ({article_data['publisher']}) {article_data['title'][:50]}...")
        
    except Exception as e:
        print(f"[ERROR] Kafka ë°œí–‰ ì‹¤íŒ¨: {e}")

def get_latest_articles(site):
    try:
        if not site.get('selector'):
            print(f"[ERROR] {site['name']} - {site.get('category', '')} â†’ selector ëˆ„ë½")
            return []

        res = session.get(site['url'], timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')

        if site.get("container_selector"):
            container = soup.select_one(site["container_selector"])
            if not container:
                print(f"[ERROR] {site['name']} â†’ container_selector ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            links = container.select(site["selector"])
        else:
            links = soup.select(site["selector"])

        articles = []
        for a in links:
            href = a.get('href')
            if not href or site['pattern'] not in href:
                continue

            # ì œëª© ì¶”ì¶œ
            # ì–¸ë¡ ì‚¬ë³„ë¡œ ì œëª© íƒœê·¸ ë¶„ê¸° ì²˜ë¦¬
            if site['name'] in ["ë§¤ì¼ê²½ì œ", "í—¤ëŸ´ë“œê²½ì œ", "í•œê²¨ë ˆ"]:
                if site['name'] == "ë§¤ì¼ê²½ì œ":
                    title_tag = a.select_one("h3.news_ttl")
                elif site['name'] == "í—¤ëŸ´ë“œê²½ì œ":
                    title_tag = a.select_one("p.news_title")
                elif site['name'] == "í•œê²¨ë ˆ":
                    title_tag = a.select_one("div.BaseArticleCard_title__TVFqt")
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)
            else:
                title = a.get_text(strip=True)
            full_url = urljoin(site['base_url'], href)

            img_url = None

            # ë§¤ì¼ê²½ì œ ì „ìš© ì²˜ë¦¬
            if site['name'] == "ë§¤ì¼ê²½ì œ":
                parent_container = a.find_parent('li', class_='news_node')
                if parent_container:
                    img_tag = parent_container.select_one('div.thumb_area > img')
                    if img_tag:
                        if img_tag.has_attr('data-src') and img_tag['data-src'].strip():
                            img_url = img_tag['data-src']
                        elif img_tag.has_attr('src') and img_tag['src'].strip():
                            img_url = img_tag['src']
                        img_url = urljoin(site['base_url'], img_url)
                # ë§¤ì¼ê²½ì œ ì „ìš© ì²˜ë¦¬ í›„ ì¼ë°˜ ë¡œì§ì€ íƒ€ì§€ ì•ŠìŒ
            else:
                # ì¼ë°˜ ì´ë¯¸ì§€ ì¶”ì¶œ ë¡œì§
                parent_container = a.find_parent(['li', 'div', 'article'])
                img_tag = None
                if parent_container:
                    img_tag = parent_container.select_one(site['image_selector'])
                if not img_tag:
                    img_tag = a.select_one('img')
                if not img_tag:
                    img_tag = soup.select_one(f"a[href='{href}'] img")
                if img_tag:
                    img_url = img_tag.get('data-src') or img_tag.get('src')
                    if img_url:
                        img_url = urljoin(site['base_url'], img_url)

            # 4ï¸âƒ£ ë°±ì—…: data-share-img ì†ì„± í™•ì¸
            if not img_url:
                share_div = parent_container.select_one("div[class^='share-data-']")
                if share_div and share_div.has_attr("data-share-img"):
                    img_url = share_div["data-share-img"]




            articles.append({
                'title': title,
                'url': full_url,
                'image': img_url if img_url else None  # âœ… ì´ë¯¸ì§€ ì—†ìœ¼ë©´ None
            })

        return articles
    except Exception as e:
        print(f"[ERROR] ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨ ({site['name']}) â†’ {e}")
        return []


def monitor_news():
    print("\n[ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘]")
    print(f"  ê¸°ì¤€ ì‹œê°„: {SCRIPT_START_TIME.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  Kafka ì„¤ì •: {kafka_config['bootstrap.servers']}")
    first_run = True

    while True:
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(get_latest_articles, site): site for site in TARGET_SITES if not site.get('dynamic')}

            for future in as_completed(futures):
                site = futures[future]
                try:
                    article_list = future.result()
                except Exception as e:
                    print(f"[ERROR] {site['name']} ì²˜ë¦¬ ì‹¤íŒ¨ â†’ {e}")
                    continue

                site_key = f"{site['name']} - {site.get('category', '')}"
                if not article_list:
                    continue
                

                top_article = article_list[0]
                top_url = top_article['url']
                top_title = top_article['title']
                top_image = top_article['image']  # âœ… ì´ë¯¸ì§€ URL ì¶”ì¶œ

                if first_run:
                    last_seen_urls[site_key] = top_url
                    print(f"  ğŸ”¹ [{site_key}] ì´ˆê¸° ê¸°ì¤€ ê¸°ì‚¬ ì„¤ì •: {top_title}")
                    continue

                if top_url == last_seen_urls[site_key]:
                    continue

                details = fetch_article_details(top_url, site=site)
                if not details:
                    continue

                pub_time = localize_to_kst(details['publish_time']) if details['publish_time'] else datetime.now(KST)

                if pub_time < SCRIPT_START_TIME:
                    continue

                if top_title in recent_titles or top_url in processed_urls:
                    continue

                recent_titles.append(top_title)
                processed_urls.append(top_url)

                # ê¸°ì—… íƒœê¹…
                company_name, max_score, _ = identify_company(details['title'], details['text'])
                company_code = None if company_name in ["ë¯¸êµ­", "ì½”ìŠ¤í”¼", "ë‹¤ë¥¸ê¸°ì—…"] else STOCK_CODE.get(company_name)

                # âœ… Kafka ë°œí–‰ ë°ì´í„° (ì´ë¯¸ì§€ URL ì¶”ê°€)
                article_data = {
                    "publisher": site['name'],
                    "category": site.get('category', 'N/A'),
                    "title": details['title'],
                    "published_date": pub_time.strftime('%Y-%m-%d %H:%M:%S'),
                    "content": details['text'],
                    "url": top_url,
                    "image_url": top_image if top_image else None,  # âœ… ì´ë¯¸ì§€ URL ì¶”ê°€
                    "company_tag": company_name,
                    "company_score": max_score,
                    "stock_code": company_code,
                    "crawled_at": datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')
                }

                send_article_to_kafka(article_data)
                last_seen_urls[site_key] = top_url
                print(f"  âœ… ê¸°ì¤€ ê¸°ì‚¬ ì—…ë°ì´íŠ¸: {site_key} â†’ {top_title[:50]}... (ì´ë¯¸ì§€: { 'ìˆìŒ' if top_image else 'ì—†ìŒ'})")

        first_run = False
        print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ë‹¤ìŒ ì²´í¬ê¹Œì§€ {CHECK_INTERVAL_SECONDS}ì´ˆ ëŒ€ê¸°...")
        time.sleep(CHECK_INTERVAL_SECONDS)


if __name__ == "__main__":
    try:
        monitor_news()
    except KeyboardInterrupt:
        print("\n[ì¢…ë£Œ] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"[ERROR] ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    finally:
        print("\n[ì •ë¦¬] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        try:
            producer.flush(timeout=10)  # ë‚¨ì€ ë©”ì‹œì§€ ì „ì†¡
            client.close()
            print("  MongoDB ì—°ê²° ì¢…ë£Œ ì™„ë£Œ")
            print("  Kafka Producer ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            print(f"[ERROR] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")